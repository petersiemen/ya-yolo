{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Yolo with MS Coco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('./src'))\n",
    "\n",
    "\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "\n",
    "from datasets.preprocess import *\n",
    "from datasets.yayolo_coco_dataset import YaYoloCocoDataset\n",
    "\n",
    "from yolo.yolo import Yolo\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=11.92s)\n",
      "creating index...\n",
      "index created!\n",
      "torch.Size([2, 3, 416, 416])\n",
      "Loading weights. Please Wait...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg_file = './cfg/yolov3.cfg'\n",
    "weight_file = './cfg/yolov3.weights'\n",
    "namesfile = './cfg/coco.names'\n",
    "batch_size = 2\n",
    "#COCO_IMAGES_DIR = '/home/peter/datasets/coco-small/cocoapi/images/train2014'\n",
    "COCO_IMAGES_DIR = '/home/ubuntu/datasets/coco/train2014'\n",
    "#COCO_ANNOTATIONS_FILE = '/home/peter/datasets/coco-small/cocoapi/annotations/instances_train2014_10_per_category.json'\n",
    "COCO_ANNOTATIONS_FILE = '/home/ubuntu/datasets/coco/annotations/instances_train2014.json'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_and_target_transform = Compose([\n",
    "        ConvertXandYToCenterOfBoundingBox(),\n",
    "        AbsoluteToRelativeBoundingBox(),\n",
    "        SquashResize(416),\n",
    "        CocoToTensor()\n",
    "    ])\n",
    "\n",
    "ya_yolo_dataset = YaYoloCocoDataset(images_dir=COCO_IMAGES_DIR, \n",
    "                                    annotations_file=COCO_ANNOTATIONS_FILE,\n",
    "                                    transforms=image_and_target_transform,\n",
    "                                    batch_size=batch_size)\n",
    "trainloader = torch.utils.data.DataLoader(ya_yolo_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "images, targets, image_paths = next(iter(trainloader))\n",
    "print(images.shape)\n",
    "\n",
    "\n",
    "model = Yolo(cfg_file=cfg_file,namesfile=namesfile, batch_size=batch_size)\n",
    "model.load_weights(weight_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  82783\n",
      "\n",
      "---- [Epoch 1/2, Batch 0/41392] ----\n",
      "+-----------------------+------------------+---------------------+---------------------+--------------------+\n",
      "| Localization          | Objectness       | No Objectness       | Classification      | Total Loss         |\n",
      "+-----------------------+------------------+---------------------+---------------------+--------------------+\n",
      "| 0.0025359743740409613 | 0.99821937084198 | 0.03259998559951782 | 0.01584632694721222 | 1.0430455207824707 |\n",
      "+-----------------------+------------------+---------------------+---------------------+--------------------+\n",
      "\n",
      "---- [Epoch 1/2, Batch 10/41392] ----\n",
      "+-----------------------+------------------------+-----------------------+----------------------+---------------------+\n",
      "| Localization          | Objectness             | No Objectness         | Classification       | Total Loss          |\n",
      "+-----------------------+------------------------+-----------------------+----------------------+---------------------+\n",
      "| 0.0003151757118757814 | 0.00040519042522646487 | 0.0008163910824805498 | 0.049761831760406494 | 0.05215109512209892 |\n",
      "+-----------------------+------------------------+-----------------------+----------------------+---------------------+\n",
      "\n",
      "---- [Epoch 1/2, Batch 20/41392] ----\n",
      "+-----------------------+--------------------+------------------------+---------------------+--------------------+\n",
      "| Localization          | Objectness         | No Objectness          | Classification      | Total Loss         |\n",
      "+-----------------------+--------------------+------------------------+---------------------+--------------------+\n",
      "| 0.0050487639382481575 | 0.9982113838195801 | 0.00030270166462287307 | 0.01136542484164238 | 1.0349719524383545 |\n",
      "+-----------------------+--------------------+------------------------+---------------------+--------------------+\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7aeeb9836625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m          \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m          limit=None, debug=False, print_every=10)\n\u001b[0m",
      "\u001b[0;32m~/workspace/ya-yolo/src/yolo/training.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, ya_yolo_dataset, model_dir, summary_writer, epochs, lr, lambda_coord, lambda_no_obj, limit, debug, print_every)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;31m# print loss statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/ya-yolo/.venv/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from yolo.training import training\n",
    "lr = 0.001\n",
    "lambda_coord=5\n",
    "lambda_no_obj=0.5\n",
    "\n",
    "summary_writer = SummaryWriter(comment=f' batch_size={batch_size} lr={lr}')\n",
    "\n",
    "training(model=model, ya_yolo_dataset=ya_yolo_dataset, model_dir='./models',\n",
    "         summary_writer=summary_writer, \n",
    "         lambda_coord=lambda_coord,\n",
    "         lambda_no_obj=lambda_no_obj,\n",
    "         epochs=2,\n",
    "         lr=lr,\n",
    "         limit=None, debug=False, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
